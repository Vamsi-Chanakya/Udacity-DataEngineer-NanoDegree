# Project Name - Data Lake Project with Spark and AWS

## Project Purpose
This project is created for a music streaming startup, called sparkify who has grown their user base vastly and want to move their data warehouse to data lake. Sparkify log data and songs data resides in S3, in form of JSON files. As a data engineer we built this ETL pipeline, which extracts their data from s3, process them using spark and load the data back to s3 as a set of dimensional tables, which enables their analytical team to find insights on populat songs and artists etc. 

## Project Description
In this project we used spark and build ETL pipeline for a data lake hosted on S3. We load the data from S3, process the data into analytical tables using spark and we load the data back to S3 inform of dimensional tables. In this project we deploy spark process on a cluster using AWS. 

## Source(s)
In this project, the data is extracted from files which are of JSON file format on s3.

### Song Dataset
Song dataset contains the metadata about a song, i.e details like artist_id, artist_latitude, artist_location, artist_longitude, artist_name, duration, num_songs, song_id, title, year. Song Dataset is partitioned by first 3 letters of each song's track id.

SONG_DATA='s3://udacity-dend/song_data'

### Log Dataset
Log Dataset is generated by event simulator based on songs in song dataset. The data is similar to music streaming app activity log and it contains the below mentioned details. Log dataset contains data like artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId.

LOG_DATA='s3://udacity-dend/log_data'

##Target(s)
The data extracted from Source JSON files are converted into spark dataframes. Using the song and log datasets, a star schema is created which is optimized for queries on song play analysis. This includes creation of the following parquet files.

### Intermediate DataFrames:

log_df - This dataframe contains app activity log information like artist, auth, first_name varchar, gender, item_in_session, last_name, length, level, location, method, page, registration, session_id, song, status, time_stamp, user_agent, user_id

song_df  - This staging table contains metadata about songs and respective artists information like num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year.

## Star Schema Details : 

![Star%20Schema%20for%20Data%20Modeling](Star%20Schema.png)

### Dimensions Parquet files :
users   - users in the app contains user details like user_id, first_name, last_name, gender, level.
songs   - songs in music database contains user details like song_id, title, artist_id, year, duration
artists - artists in music database contains user details like artist_id, name, location, lattitude, longitude
time    - timestamps of records in songplays broken down into specific units, contains user details like start_time, hour, day, week, month, year, weekday

Fact Parquet File
songplays - records in log data associated with song plays i.e. records with page NextSong,  contains user details like songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

## ETL process :

### Extraction Process:
Extraction process is comprised of extracting log and song data from S3 and creating 2 staging dataframes which are log_df and song_df.

### Transformation process:
In this stage we transform log and song details into dimension data frames, which are users, songs, artists and time.

### Load process:
In this stage we create final Fact dataframe which is SongPlays by consolidating data from dimension tables and write it back to S3. 


## Execution Order:
etl.py : This step extracts data from AWS S3 and inserts the data into staging, dimension and fact dataframes and writes back the fact df to S3. 

## Constraints:
Data Quality needs to be improved. 
Null value records should be filtered.
Duplicates should be eliminated from dimension dataframes like songs, artists, time and users.
Dont execute notebook commands with full data.

## References:
1. https://www.python.org/dev/peps/pep-0008/#introduction
2. https://spark.apache.org/docs/2.4.3/
3. https://stackoverflow.com/
4. https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-dg.pdf
5. https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html
6. https://docs.aws.amazon.com/emr/


