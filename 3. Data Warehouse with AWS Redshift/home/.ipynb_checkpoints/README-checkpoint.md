Project Name
Data Warehouse with AWS RedShift

Project Purpose
This project is created for a music streaming startup, called sparkify who has grown their user base and song database and want to migrate their ETL processes and data onto the cloud. Sparkify data resides in S3, in form of JSON files.

Introduction
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

Project Description
In this project we are building an ETL pipeline that extracts sparkify data from S3, stages in Redshift into staging tables staging_events and staging_songs, and transforms the data into a set of dimensional tables users, songs, artists and time, for sparkify analytics team to continue finding insights in what songs their users are listening to most and which ones are least heard.

Source(s)
In this project, the data is extracted from files which are of JSON file format.

Song Dataset
Song dataset contains the metadata about a song, i.e details like artist_id, artist_latitude, artist_location, artist_longitude, artist_name, duration, num_songs, song_id, title, year. Song Dataset is partitioned by first 3 letters of each song's track id.

SONG_DATA='s3://udacity-dend/song_data'

Log Dataset
Log Dataset is generated by event simulator based on songs in song dataset. The data is similar to music streaming app activity log and it contains the below mentioned details. Log dataset contains data like artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId.

LOG_DATA='s3://udacity-dend/log_data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'

Target(s)
The data extracted from Source JSON files are loaded into a new Redshift database, named as dwh. Using the song and log datasets, a star schema is created which is optimized for queries on song play analysis. This includes the following tables.

Staging Tables:

staging_events - This staging table contains app activity log information like artist, auth, first_name varchar, gender, item_in_session, last_name, length, level, location, method, page, registration, session_id, song, status, time_stamp, user_agent, user_id

staging_songs  - This staging table contains metadata about songs and respective artists information like num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year.

Star Schema Details : 

Dimension Tables:
users   - users in the app contains user details like user_id, first_name, last_name, gender, level.
songs   - songs in music database contains user details like song_id, title, artist_id, year, duration
artists - artists in music database contains user details like artist_id, name, location, lattitude, longitude
time    - timestamps of records in songplays broken down into specific units, contains user details like start_time, hour, day, week, month, year, weekday

Fact Table
songplays - records in log data associated with song plays i.e. records with page NextSong,  contains user details like songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

ETL process :

Extraction Process:
Step 1: ETL process is comprised of creating 2 staging tables which are staging_events table and staging_songs table, 4 dimension tables which are users, songs, artists and time tables, and finally one fact table sonplays . 
Step 2: In second step we extract the data into staging tables on AWS Redshift from S3 bucket.

Transformation process:
In this stage we transform staging data of events and songs details into dimension tables users, songs, artists and time.

Load process:
In this stage we load the Fact Table SongPlays table by consolidating data from dimension tables. 


Execution Order:
create_tables.py : This step drops if the staging, dimesion and fact tables if they already exists and creates them.
etl.py : This step extracts data from AWS S3 and inserts the data into staging, dimension and fact tables on AWS Redshift Database. 

Number of Records Loaded into each table:
numberOfRows in table staging_events are 8056
numberOfRows in table staging_songs are 14896
numberOfRows in table users are 97
numberOfRows in table songs are 14896
numberOfRows in table artists are 10025
numberOfRows in table time are 8023
numberOfRows in table songplays are 6820

Constraints:
Data Quality needs to be improved as we have values for all records.
After creating Redshift Cluster we need to wait until it becomes available
Emptied HOST, DB_USER, DB_PASSWORD and ARN from dwh.cfg for security reasons

References:
1. https://www.python.org/dev/peps/pep-0008/#introduction
2. https://www.w3schools.com/ 
3. https://stackoverflow.com/
4. https://docs.aws.amazon.com/redshift/index.html
5. https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html


